[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francis Chan",
    "section": "",
    "text": "I’m a senior at UCLA majoring in Statistics & Data Science. I enjoy solving problem, tackling complex challenges, and leveraging data to drive impactful descisions."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Francis Chan",
    "section": "Education",
    "text": "Education\nUniversity of California, Los Angeles | Los Angeles, CA\nB.S in Statistics & Data Science | Sept 2023 - June 2025\nDe Anza College | Cupertino, CA\nA.S in Mathematics | Sept 2021 - June 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Francis Chan",
    "section": "Experience",
    "text": "Experience\nUCLA Housing | Front Desk Assistant\nJune 2024 - present"
  },
  {
    "objectID": "index.html#skill-sets",
    "href": "index.html#skill-sets",
    "title": "Francis Chan",
    "section": "Skill sets",
    "text": "Skill sets\n\nPython\n\nNumPy\nPandas\nMatplotlib\nScikit-learn\nTensorFlow\n\nR\n\nggplot2\nDplyr\nCaret\n\nSQL\nTableau\nMS Excel\n\nVLookup\nPivot Table\n\nStata\nGit\n\nGithub\n\nJava\nHTML5\nCSS\nC++"
  },
  {
    "objectID": "projects/project_3.html",
    "href": "projects/project_3.html",
    "title": "2023-2024 NBA Results Prediction",
    "section": "",
    "text": "This project aimed to predict the outcomes of NBA games during the 2023-2024 season using past game statistics. The objective was to develop a statistical model capable of estimating game results based on historical performance, team matchups, and engineered features. The primary challenge was structuring the data for predictive modeling, which involved extensive feature engineering and selection techniques.\n\n\n\n\nData Preprocessing:\n\nCleaned the dataset of 1,230 games, ensuring statistical integrity.\nReshaped data so each entry contained all relevant statistics for a single game rather than separate team-specific entries.\nStandardized numerical features to maintain consistency across different scales.\n\nFeature Engineering:\n\nCreated new variables such as block percentage and assist percentage to capture team performance more effectively.\nIntroduced a team instability metric, measuring performance variance across games to assess consistency.\nComputed weighted mean past statistics, giving greater importance to recent games, home/away status, and previous matchups.\nUsed relative statistical measures (e.g., raw difference, squared difference) to better contextualize team performance.\n\nFeature Selection:\n\nApplied L1-regularized Logistic Regression to identify the most informative variables.\nFurther refined the feature set through Principal Component Analysis (PCA) and forward selection, ultimately reducing the model to two key variables:\nA principal component combining point difference and field goal percentage.\nThe raw count of successful three-point shots.\n\nModeling and Evaluation:\n\nExperimented with various predictive models, including:\n\nLogistic Regression\nLinear and Radial Support Vector Machines (SVM)\nK-Nearest Neighbors (KNN)\nLinear and Quadratic Discriminant Analysis (LDA, QDA)\nRandom Forest\n\nTuned hyperparameters via grid search and cross-validation to optimize accuracy.\nFound Linear SVM to be the best-performing model, achieving a validation accuracy of 68.7%.\n\n\n\n\n\n\nSuccessfully built a predictive model with nearly 70% accuracy using only two key features.\nIdentified home/away performance weighting as a significant factor in prediction accuracy.\nDetermined that Random Forest performed poorly due to excessive feature reduction, which limited its effectiveness.\n\n\n\n\n\nThe current model relies on team-level statistics; incorporating individual player data and roster changes could improve predictions.\nPlaystyle categorization (e.g., defensive vs. offensive strategies) may enhance feature selection.\nExploring alternative matchup weighting schemes (e.g., team similarity metrics) could refine performance estimates.\nFurther expanding the dataset beyond a single season might enhance model generalizability.\n\n\n\n\n2023-2024 NBA Results Prediction"
  },
  {
    "objectID": "projects/project_3.html#summary",
    "href": "projects/project_3.html#summary",
    "title": "2023-2024 NBA Results Prediction",
    "section": "",
    "text": "This project aimed to predict the outcomes of NBA games during the 2023-2024 season using past game statistics. The objective was to develop a statistical model capable of estimating game results based on historical performance, team matchups, and engineered features. The primary challenge was structuring the data for predictive modeling, which involved extensive feature engineering and selection techniques.\n\n\n\n\nData Preprocessing:\n\nCleaned the dataset of 1,230 games, ensuring statistical integrity.\nReshaped data so each entry contained all relevant statistics for a single game rather than separate team-specific entries.\nStandardized numerical features to maintain consistency across different scales.\n\nFeature Engineering:\n\nCreated new variables such as block percentage and assist percentage to capture team performance more effectively.\nIntroduced a team instability metric, measuring performance variance across games to assess consistency.\nComputed weighted mean past statistics, giving greater importance to recent games, home/away status, and previous matchups.\nUsed relative statistical measures (e.g., raw difference, squared difference) to better contextualize team performance.\n\nFeature Selection:\n\nApplied L1-regularized Logistic Regression to identify the most informative variables.\nFurther refined the feature set through Principal Component Analysis (PCA) and forward selection, ultimately reducing the model to two key variables:\nA principal component combining point difference and field goal percentage.\nThe raw count of successful three-point shots.\n\nModeling and Evaluation:\n\nExperimented with various predictive models, including:\n\nLogistic Regression\nLinear and Radial Support Vector Machines (SVM)\nK-Nearest Neighbors (KNN)\nLinear and Quadratic Discriminant Analysis (LDA, QDA)\nRandom Forest\n\nTuned hyperparameters via grid search and cross-validation to optimize accuracy.\nFound Linear SVM to be the best-performing model, achieving a validation accuracy of 68.7%.\n\n\n\n\n\n\nSuccessfully built a predictive model with nearly 70% accuracy using only two key features.\nIdentified home/away performance weighting as a significant factor in prediction accuracy.\nDetermined that Random Forest performed poorly due to excessive feature reduction, which limited its effectiveness.\n\n\n\n\n\nThe current model relies on team-level statistics; incorporating individual player data and roster changes could improve predictions.\nPlaystyle categorization (e.g., defensive vs. offensive strategies) may enhance feature selection.\nExploring alternative matchup weighting schemes (e.g., team similarity metrics) could refine performance estimates.\nFurther expanding the dataset beyond a single season might enhance model generalizability.\n\n\n\n\n2023-2024 NBA Results Prediction"
  },
  {
    "objectID": "projects/project_1.html",
    "href": "projects/project_1.html",
    "title": "Banknote Authentication and Model Evaluation",
    "section": "",
    "text": "This project aimed to classify banknotes as genuine or counterfeit using machine learning models. The objective was to develop and compare different classifiers to determine the most accurate model for this task. The process involved data preprocessing, model training, evaluation, and performance analysis.\n\n\n\n\nData Preparation:\n\nLoaded the dataset and structured it into feature variables (X) and labels (y).\nStandardized feature variables for improved model performance.\nSplit the dataset into training and testing sets using a 70-30 ratio.\n\nModeling and Evaluation:\n\nTrained a Decision Tree classifier and evaluated its accuracy on both training and test sets.\nImplemented a Random Forest model with 51 estimators and a depth of 5, achieving higher accuracy compared to the Decision Tree.\nApplied a Gradient Boosting classifier with the same number of estimators and depth, which showed the highest training accuracy.\nAssessed model performance using accuracy scores and test error rates.\n\n\n\n\n\n\nThe Decision Tree model had a test accuracy of 96.36%, indicating moderate performance.\nThe Random Forest model improved accuracy to 99.51%, showing better generalization.\nThe Gradient Boosting model achieved a near-perfect accuracy of 99.27%, suggesting strong predictive capabilities.\nBoth Random Forest and Gradient Boosting significantly outperformed the Decision Tree in terms of generalization and error minimization.\nThis analysis demonstrated the effectiveness of ensemble methods like Random Forest and Gradient Boosting in reducing classification errors for banknote authentication.\n\n\n\n\nBanknote Authentication"
  },
  {
    "objectID": "projects/project_1.html#summary",
    "href": "projects/project_1.html#summary",
    "title": "Banknote Authentication and Model Evaluation",
    "section": "",
    "text": "This project aimed to classify banknotes as genuine or counterfeit using machine learning models. The objective was to develop and compare different classifiers to determine the most accurate model for this task. The process involved data preprocessing, model training, evaluation, and performance analysis.\n\n\n\n\nData Preparation:\n\nLoaded the dataset and structured it into feature variables (X) and labels (y).\nStandardized feature variables for improved model performance.\nSplit the dataset into training and testing sets using a 70-30 ratio.\n\nModeling and Evaluation:\n\nTrained a Decision Tree classifier and evaluated its accuracy on both training and test sets.\nImplemented a Random Forest model with 51 estimators and a depth of 5, achieving higher accuracy compared to the Decision Tree.\nApplied a Gradient Boosting classifier with the same number of estimators and depth, which showed the highest training accuracy.\nAssessed model performance using accuracy scores and test error rates.\n\n\n\n\n\n\nThe Decision Tree model had a test accuracy of 96.36%, indicating moderate performance.\nThe Random Forest model improved accuracy to 99.51%, showing better generalization.\nThe Gradient Boosting model achieved a near-perfect accuracy of 99.27%, suggesting strong predictive capabilities.\nBoth Random Forest and Gradient Boosting significantly outperformed the Decision Tree in terms of generalization and error minimization.\nThis analysis demonstrated the effectiveness of ensemble methods like Random Forest and Gradient Boosting in reducing classification errors for banknote authentication.\n\n\n\n\nBanknote Authentication"
  },
  {
    "objectID": "projects/project_2.html",
    "href": "projects/project_2.html",
    "title": "MNIST Handwritten Digit Recognition",
    "section": "",
    "text": "This project focused on improving the accuracy of handwritten digit recognition using the MNIST dataset. The objective was to enhance an existing model’s performance by implementing various optimization techniques. The goal was to increase accuracy from an initial baseline of 95% to nearly 99%.\n\n\n\n\nModel Optimization:\n\nUsed a pre-trained model as a baseline for comparison.\nAdjusted the MaxPooling2D layer’s strides parameter to (1,1) to reduce downsampling and capture more features.\nModified the Dropout rate to 0.05, ensuring more neurons remained active during training while preventing overfitting.\n\nTechniques Implemented:\n\nExperimented with different batch sizes and number of epochs to optimize training.\nEvaluated the effect of GPU vs. CPU performance on model training speed.\n\n\n\n\n\n\nSuccessfully improved the model’s accuracy from 95% to nearly 99%.\nApplied fine-tuned hyperparameters to create an optimized model with enhanced digit recognition capabilities.\nBalanced model complexity and generalization through strategic dropout settings and pooling adjustments.\n\n\n\n\nMNIST Handwritten Digit Recognition"
  },
  {
    "objectID": "projects/project_2.html#summary",
    "href": "projects/project_2.html#summary",
    "title": "MNIST Handwritten Digit Recognition",
    "section": "",
    "text": "This project focused on improving the accuracy of handwritten digit recognition using the MNIST dataset. The objective was to enhance an existing model’s performance by implementing various optimization techniques. The goal was to increase accuracy from an initial baseline of 95% to nearly 99%.\n\n\n\n\nModel Optimization:\n\nUsed a pre-trained model as a baseline for comparison.\nAdjusted the MaxPooling2D layer’s strides parameter to (1,1) to reduce downsampling and capture more features.\nModified the Dropout rate to 0.05, ensuring more neurons remained active during training while preventing overfitting.\n\nTechniques Implemented:\n\nExperimented with different batch sizes and number of epochs to optimize training.\nEvaluated the effect of GPU vs. CPU performance on model training speed.\n\n\n\n\n\n\nSuccessfully improved the model’s accuracy from 95% to nearly 99%.\nApplied fine-tuned hyperparameters to create an optimized model with enhanced digit recognition capabilities.\nBalanced model complexity and generalization through strategic dropout settings and pooling adjustments.\n\n\n\n\nMNIST Handwritten Digit Recognition"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n2023-2024 NBA Results Prediction\n\n\n\n\n\n\nPython\n\n\nPrediction\n\n\n\nThis project involves preprocessing and predicting the NBA dataset.\n\n\n\n\n\nDec 13, 2024\n\n\nFrancis Chan\n\n\n\n\n\n\n\n\n\n\n\n\nBanknote Authentication and Model Evaluation\n\n\n\n\n\n\nPython\n\n\nClassification\n\n\n\nThis project involves classifying genuine or counterfeit banknotes.\n\n\n\n\n\nNov 10, 2024\n\n\nFrancis Chan\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST Handwritten Digit Recognition\n\n\n\n\n\n\nPython\n\n\nClassification\n\n\n\nThis project aims to improve the classification model.\n\n\n\n\n\nAug 17, 2024\n\n\nFrancis Chan\n\n\n\n\n\n\nNo matching items"
  }
]